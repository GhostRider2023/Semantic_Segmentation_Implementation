{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6927,"databundleVersionId":45059,"sourceType":"competition"},{"sourceId":13059254,"sourceType":"datasetVersion","datasetId":8269840},{"sourceId":13059479,"sourceType":"datasetVersion","datasetId":8269990}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ======================\nimport os\nimport torchvision.transforms.functional as TF\nimport numpy as np\nfrom PIL import Image, ImageDraw, ImageOps\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda.amp import autocast, GradScaler\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T22:06:32.799363Z","iopub.execute_input":"2025-09-14T22:06:32.799718Z","iopub.status.idle":"2025-09-14T22:06:35.021446Z","shell.execute_reply.started":"2025-09-14T22:06:32.799699Z","shell.execute_reply":"2025-09-14T22:06:35.020643Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"\n# ======================\n# Config\n# ======================\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nIMAGE_HEIGHT = 1000\nIMAGE_WIDTH = 1918\nBATCH_SIZE = 1\nLEARNING_RATE = 1e-4\nNUM_EPOCHS = 5\nNUM_WORKERS = 2\nPIN_MEMORY = True\nLOAD_MODEL = True\n\n# Change these to your Kaggle dataset paths\nTRAIN_IMG_DIR = \"/kaggle/input/unet-implementation-carvana-dataset/train_Carvana/New folder\"\nTRAIN_MASK_DIR = \"/kaggle/input/unet-implementation-carvana-dataset/train_mask_carvana/train_mask_carvana\"\nVALID_IMG_DIR = \"/kaggle/input/unet-implementation-carvana-dataset/val_carvana/val_carvana\"\nVALID_MASK_DIR = \"/kaggle/input/unet-implementation-carvana-dataset/val_mask_carvana/val_mask_carvana\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T22:08:43.978977Z","iopub.execute_input":"2025-09-14T22:08:43.979800Z","iopub.status.idle":"2025-09-14T22:08:43.984584Z","shell.execute_reply.started":"2025-09-14T22:08:43.979767Z","shell.execute_reply":"2025-09-14T22:08:43.983973Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# ======================\n# Dataset\n# ======================\nclass CarvanaDataset(Dataset):\n    def __init__(self, img_dir, mask_dir, transform=None):\n        self.img_dir = img_dir\n        self.mask_dir = mask_dir\n        self.transform = transform\n        self.imgs = os.listdir(img_dir)\n\n    def __len__(self):\n        return len(self.imgs)\n\n    def __getitem__(self, idx):\n        img_path = os.path.join(self.img_dir, self.imgs[idx])\n        mask_path = os.path.join(self.mask_dir, self.imgs[idx].replace(\".jpg\", \"_mask.gif\"))\n\n        img = np.array(Image.open(img_path).convert(\"RGB\"))\n        mask = np.array(Image.open(mask_path).convert(\"L\"), dtype=np.float32)\n        mask[mask == 255.0] = 1.0\n\n        if self.transform is not None:\n            aug = self.transform(image=img, mask=mask)\n            img = aug[\"image\"]\n            mask = aug[\"mask\"]\n\n        return img, mask","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T22:06:40.393619Z","iopub.execute_input":"2025-09-14T22:06:40.394248Z","iopub.status.idle":"2025-09-14T22:06:40.400098Z","shell.execute_reply.started":"2025-09-14T22:06:40.394200Z","shell.execute_reply":"2025-09-14T22:06:40.399504Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"\n#  -------------------------PSEUDO ARCHITECHTURE_____________________________________\n# Input Image:  3 × 1000 × 1918  \n\n# ---------- Downsampling path (encoder) ----------\n# 1. DoubleConv (3 → 64)      → 64 × 1000 × 1918\n# 2. MaxPool(2×2)             → 64 × 500 × 959\n\n# 3. DoubleConv (64 → 128)    → 128 × 500 × 959\n# 4. MaxPool(2×2)             → 128 × 250 × 479\n\n# 5. DoubleConv (128 → 256)   → 256 × 250 × 479\n# 6. MaxPool(2×2)             → 256 × 125 × 239\n\n# 7. DoubleConv (256 → 512)   → 512 × 125 × 239\n# 8. MaxPool(2×2)             → 512 × 62 × 119\n\n# ---------- Bottleneck ----------\n# 9. DoubleConv (512 → 1024)  → 1024 × 62 × 119\n\n# ---------- Upsampling path (decoder) ----------\n# 10. ConvTranspose (1024 → 512, stride=2)  → 512 × 124 × 238\n#     Concatenate with encoder skip (512 × 125 × 239) → resized and concat → 1024 × 125 × 239\n#     DoubleConv (1024 → 512) → 512 × 125 × 239\n\n# 11. ConvTranspose (512 → 256, stride=2)   → 256 × 250 × 478\n#     Concatenate with encoder skip (256 × 250 × 479) → concat → 512 × 250 × 479\n#     DoubleConv (512 → 256) → 256 × 250 × 479\n\n# 12. ConvTranspose (256 → 128, stride=2)   → 128 × 500 × 958\n#     Concatenate with encoder skip (128 × 500 × 959) → concat → 256 × 500 × 959\n#     DoubleConv (256 → 128) → 128 × 500 × 959\n\n# 13. ConvTranspose (128 → 64, stride=2)    → 64 × 1000 × 1918\n#     Concatenate with encoder skip (64 × 1000 × 1918) → concat → 128 × 1000 × 1918\n#     DoubleConv (128 → 64) → 64 × 1000 × 1918\n\n# ---------- Final output ----------\n# 14. Final Conv (64 → 1, kernel=1) → 1 × 1000 × 1918  (binary mask)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ======================\n# UNet Model\n# ======================\nclass DoubleConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(DoubleConv, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n        )\n\n    def forward(self, x):\n        return self.conv(x)\n\n\nclass UNet(nn.Module):\n    def __init__(self, in_channels=3, out_channels=1, features=[64,128,256,512]):\n        super(UNet, self).__init__()\n        self.downs = nn.ModuleList()\n        self.ups = nn.ModuleList()\n        self.pool = nn.MaxPool2d(2,2)\n\n        # Down\n        for feature in features:\n            self.downs.append(DoubleConv(in_channels, feature))\n            in_channels = feature\n\n        # Up\n        for feature in reversed(features):\n            self.ups.append(\n                nn.ConvTranspose2d(feature*2, feature, kernel_size=2, stride=2)\n            )\n            self.ups.append(DoubleConv(feature*2, feature))\n\n        self.bottleneck = DoubleConv(features[-1], features[-1]*2)\n        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n\n    def forward(self, x):\n        skip_connections = []\n        for down in self.downs:\n            x = down(x)\n            skip_connections.append(x)\n            x = self.pool(x)\n\n        x = self.bottleneck(x)\n        skip_connections = skip_connections[::-1]\n\n        for idx in range(0, len(self.ups), 2):\n            x = self.ups[idx](x)\n            skip_connection = skip_connections[idx//2]\n            if x.shape != skip_connection.shape:\n                x = torchvision.transforms.functional.resize(x, size=skip_connection.shape[2:])\n            concat_skip = torch.cat((skip_connection, x), dim=1)\n            x = self.ups[idx+1](concat_skip)\n\n        return self.final_conv(x)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ======================\n# Utils\n# ======================\ndef save_checkpoint(state, filename=\"/kaggle/working/checkpoint.pth.tar\"):\n    print(\"=> Saving checkpoint\")\n    torch.save(state, filename)\n\ndef load_checkpoint(checkpoint, model):\n    print(\"=> Loading checkpoint\")\n    model.load_state_dict(checkpoint[\"state_dict\"])\n\ndef get_loaders(train_dir, train_mask_dir, val_dir, val_mask_dir,\n                batch_size, train_transform, val_transform,\n                num_workers=4, pin_memory=True):\n\n    train_ds = CarvanaDataset(train_dir, train_mask_dir, transform=train_transform)\n    val_ds = CarvanaDataset(val_dir, val_mask_dir, transform=val_transform)\n\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,\n                              num_workers=num_workers, pin_memory=pin_memory)\n    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False,\n                            num_workers=num_workers, pin_memory=pin_memory)\n    return train_loader, val_loader\n\ndef check_accuracy(loader, model, device=DEVICE):\n    num_correct = 0\n    num_pixels = 0\n    dice_score = 0\n    model.eval()\n    with torch.no_grad():\n        for x, y in loader:\n            x = x.to(device)\n            y = y.to(device).unsqueeze(1)\n            preds = torch.sigmoid(model(x))\n            preds = (preds > 0.5).float()\n            num_correct += (preds == y).sum()\n            num_pixels += torch.numel(preds)\n            dice_score += (2 * (preds * y).sum()) / ((preds + y).sum() + 1e-8)\n\n    print(f\"Accuracy: {num_correct/num_pixels*100:.2f}%\")\n    print(f\"Dice score: {dice_score/len(loader)}\")\n    model.train()\n\ndef save_predictions_as_imgs(loader, model, folder=\"/kaggle/working/saved_images\", device=DEVICE):\n    os.makedirs(folder, exist_ok=True)\n    model.eval()\n    for idx, (x, y) in enumerate(loader):\n        x = x.to(device)\n        with torch.no_grad():\n            preds = torch.sigmoid(model(x))\n            preds = (preds > 0.5).float()\n        torchvision.utils.save_image(preds, f\"{folder}/pred_{idx}.png\")\n        torchvision.utils.save_image(y.unsqueeze(1), f\"{folder}/original_{idx}.png\")\n    model.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T22:06:45.665794Z","iopub.execute_input":"2025-09-14T22:06:45.666056Z","iopub.status.idle":"2025-09-14T22:06:45.675319Z","shell.execute_reply.started":"2025-09-14T22:06:45.666035Z","shell.execute_reply":"2025-09-14T22:06:45.674591Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"\n# ======================\n# Training\n# ======================\ndef train_fn(loader, model, optimizer, loss_fn, scaler):\n    loop = tqdm(loader)\n    for batch_idx, (data, targets) in enumerate(loop):\n        data = data.to(DEVICE)\n        targets = targets.float().unsqueeze(1).to(DEVICE)\n\n        with autocast():\n            predictions = model(data)\n            loss = loss_fn(predictions, targets)\n\n        optimizer.zero_grad()\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        loop.set_postfix(loss=loss.item())\n\ndef main():\n    train_transform = A.Compose([\n        A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n        A.Rotate(limit=35, p=1.0),\n        A.HorizontalFlip(p=0.5),\n        A.VerticalFlip(p=0.1),\n        A.Normalize(mean=[0,0,0], std=[1,1,1], max_pixel_value=255.0),\n        ToTensorV2(),\n    ])\n    val_transform = A.Compose([\n        A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n        A.Normalize(mean=[0,0,0], std=[1,1,1], max_pixel_value=255.0),\n        ToTensorV2(),\n    ])\n\n    model = UNet(in_channels=3, out_channels=1).to(DEVICE)\n    loss_fn = nn.BCEWithLogitsLoss()\n    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\n    train_loader, val_loader = get_loaders(\n        TRAIN_IMG_DIR, TRAIN_MASK_DIR, VALID_IMG_DIR, VALID_MASK_DIR,\n        BATCH_SIZE, train_transform, val_transform, NUM_WORKERS, PIN_MEMORY\n    )\n\n    scaler = GradScaler()\n\n    for epoch in range(NUM_EPOCHS):\n        print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}]\")\n        train_fn(train_loader, model, optimizer, loss_fn, scaler)\n        checkpoint = {\"state_dict\": model.state_dict(), \"optimizer\": optimizer.state_dict()}\n        save_checkpoint(checkpoint)\n        check_accuracy(val_loader, model, device=DEVICE)\n        save_predictions_as_imgs(val_loader, model, folder=\"/kaggle/working/saved_images\", device=DEVICE)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T22:06:49.156098Z","iopub.execute_input":"2025-09-14T22:06:49.156744Z","iopub.status.idle":"2025-09-14T22:06:49.168142Z","shell.execute_reply.started":"2025-09-14T22:06:49.156711Z","shell.execute_reply":"2025-09-14T22:06:49.167445Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T22:08:47.918553Z","iopub.execute_input":"2025-09-14T22:08:47.918799Z","iopub.status.idle":"2025-09-14T22:18:18.652326Z","shell.execute_reply.started":"2025-09-14T22:08:47.918783Z","shell.execute_reply":"2025-09-14T22:18:18.651140Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_36/2550349214.py:44: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = GradScaler()\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/5]\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/48 [00:00<?, ?it/s]/tmp/ipykernel_36/2550349214.py:10: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n100%|██████████| 48/48 [00:38<00:00,  1.24it/s, loss=0.259]\n","output_type":"stream"},{"name":"stdout","text":"=> Saving checkpoint\nAccuracy: 86.96%\nDice score: 0.5736066699028015\nEpoch [2/5]\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 48/48 [00:40<00:00,  1.19it/s, loss=0.232]\n","output_type":"stream"},{"name":"stdout","text":"=> Saving checkpoint\nAccuracy: 91.19%\nDice score: 0.6705350875854492\nEpoch [3/5]\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 48/48 [00:40<00:00,  1.19it/s, loss=0.215]\n","output_type":"stream"},{"name":"stdout","text":"=> Saving checkpoint\nAccuracy: 91.22%\nDice score: 0.7006963491439819\nEpoch [4/5]\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 48/48 [00:40<00:00,  1.20it/s, loss=0.159]\n","output_type":"stream"},{"name":"stdout","text":"=> Saving checkpoint\nAccuracy: 91.96%\nDice score: 0.7038361430168152\nEpoch [5/5]\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 48/48 [00:40<00:00,  1.20it/s, loss=0.135]\n","output_type":"stream"},{"name":"stdout","text":"=> Saving checkpoint\nAccuracy: 92.26%\nDice score: 0.7308205962181091\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}